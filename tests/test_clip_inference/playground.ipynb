{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from clip_retrieval.clip_inference.reader import FilesReader, WebdatasetReader\n",
    "from clip_retrieval.clip_inference.runner import Sampler\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "from clip_retrieval.clip_inference.load_clip import load_clip\n",
    "\n",
    "images = \"test_images\"\n",
    "tars = \"test_tars\"\n",
    "folder = images\n",
    "\n",
    "batch_size = 2\n",
    "num_prepro_workers = 2\n",
    "_, preprocess = load_clip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_partition_count = 3\n",
    "for output_partition_id in range(output_partition_count):\n",
    "    print(\"output_partition_id\", output_partition_id)\n",
    "    sampler = Sampler(output_partition_id=output_partition_id, output_partition_count=output_partition_count)\n",
    "    reader = FilesReader(\n",
    "        sampler,\n",
    "        preprocess,\n",
    "        folder,\n",
    "        batch_size,\n",
    "        num_prepro_workers,\n",
    "        enable_text=False,\n",
    "        enable_image=True,\n",
    "        enable_metadata=False,\n",
    "    )\n",
    "    for i in reader:\n",
    "        print(\"hi\")\n",
    "        print(i[\"image_filename\"])\n",
    "        print(i[\"image_tensor\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn this into a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_partition_count = 2\n",
    "tars = [\"test_tars/image1.tar\", \"test_tars/image2.tar\"]\n",
    "for output_partition_id in range(output_partition_count):\n",
    "    print(\"output_partition_id\", output_partition_id)\n",
    "    sampler = Sampler(output_partition_id=output_partition_id, output_partition_count=output_partition_count)\n",
    "    reader = WebdatasetReader(\n",
    "        sampler,\n",
    "        preprocess,\n",
    "        tars,\n",
    "        batch_size,\n",
    "        num_prepro_workers,\n",
    "        enable_text=False,\n",
    "        enable_image=True,\n",
    "        enable_metadata=False,\n",
    "    )\n",
    "    for i in reader:\n",
    "        print(\"hi\")\n",
    "        print(i[\"image_filename\"])\n",
    "        print(i[\"image_tensor\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_partition_count = 2\n",
    "batch_size = 256\n",
    "num_prepro_workers = 8\n",
    "from braceexpand import braceexpand\n",
    "from tqdm import tqdm\n",
    "\n",
    "tars = [i for i in braceexpand(\"pipe:aws s3 cp s3://laion-us-east-1/laion-data/laion2B-data/{000000..231348}.tar -\")]\n",
    "for output_partition_id in range(output_partition_count):\n",
    "    print(\"output_partition_id\", output_partition_id)\n",
    "    sampler = Sampler(output_partition_id=output_partition_id, output_partition_count=output_partition_count)\n",
    "    reader = WebdatasetReader(\n",
    "        sampler,\n",
    "        preprocess,\n",
    "        tars,\n",
    "        batch_size,\n",
    "        num_prepro_workers,\n",
    "        enable_text=False,\n",
    "        enable_image=True,\n",
    "        enable_metadata=False,\n",
    "    )\n",
    "    for i in tqdm(reader):\n",
    "        pass\n",
    "        # print(\"hi\")\n",
    "        # print(i['image_filename'])\n",
    "        # print(i['image_tensor'].shape)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn this into a test as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next save the tensors, and test the mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "output_partition_count = 1\n",
    "batch_size = 2\n",
    "tars = [\"test_tars/image1.tar\", \"test_tars/image2.tar\"]\n",
    "for output_partition_id in range(output_partition_count):\n",
    "    print(\"output_partition_id\", output_partition_id)\n",
    "    sampler = Sampler(output_partition_id=output_partition_id, output_partition_count=output_partition_count)\n",
    "    reader = WebdatasetReader(\n",
    "        sampler,\n",
    "        preprocess,\n",
    "        tars,\n",
    "        batch_size,\n",
    "        num_prepro_workers,\n",
    "        enable_text=False,\n",
    "        enable_image=True,\n",
    "        enable_metadata=False,\n",
    "    )\n",
    "    for i, sample in enumerate(reader):\n",
    "        with open(\"test_tensors/{}.pkl\".format(i), \"wb\") as f:\n",
    "            pickle.dump(sample, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from clip_retrieval.clip_inference.mapper import ClipMapper\n",
    "\n",
    "mapper = ClipMapper(\n",
    "    enable_image=True,\n",
    "    enable_text=False,\n",
    "    enable_metadata=False,\n",
    "    use_mclip=False,\n",
    "    device=\"cpu\",\n",
    "    clip_model=\"ViT-B/32\",\n",
    "    use_jit=True,\n",
    "    mclip_model=\"\",\n",
    ")\n",
    "tensor_files = [i for i in os.listdir(\"test_tensors\")]\n",
    "for tensor_file in tensor_files:\n",
    "    with open(\"test_tensors/{}\".format(tensor_file), \"rb\") as f:\n",
    "        tensor = pickle.load(f)\n",
    "        sample = mapper(tensor)\n",
    "        assert sample[\"image_embs\"].shape[0] == tensor[\"image_tensor\"].shape[0]\n",
    "        with open(\"test_embeddings/{}\".format(tensor_file), \"wb\") as f:\n",
    "            pickle.dump(sample, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next save the predictions, and test the writter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from clip_retrieval.clip_inference.writer import NumpyWriter\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    writer = NumpyWriter(\n",
    "        partition_id=0,\n",
    "        output_folder=tmpdir,\n",
    "        enable_text=False,\n",
    "        enable_image=True,\n",
    "        enable_metadata=False,\n",
    "        write_batch_size=10,\n",
    "    )\n",
    "    embedding_files = [i for i in os.listdir(\"test_embeddings\")]\n",
    "    expected_shape = 0\n",
    "    for embedding_file in embedding_files:\n",
    "        with open(\"test_embeddings/{}\".format(embedding_file), \"rb\") as f:\n",
    "            embedding = pickle.load(f)\n",
    "            expected_shape += embedding[\"image_embs\"].shape[0]\n",
    "            writer(embedding)\n",
    "    writer.flush()\n",
    "\n",
    "    with open(tmpdir + \"/img_emb/img_emb_0.npy\", \"rb\") as f:\n",
    "        image_embs = np.load(f)\n",
    "        assert image_embs.shape[0] == expected_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next do a runner test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from clip_retrieval.clip_inference.load_clip import load_clip\n",
    "from clip_retrieval.clip_inference.mapper import ClipMapper\n",
    "from clip_retrieval.clip_inference.reader import FilesReader, WebdatasetReader\n",
    "from clip_retrieval.clip_inference.runner import Runner, Sampler\n",
    "from clip_retrieval.clip_inference.writer import NumpyWriter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import tempfile\n",
    "\n",
    "output_partition_count = 2\n",
    "num_prepro_workers = 8\n",
    "batch_size = 2\n",
    "folder = \"test_images\"\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "\n",
    "    def reader_builder(sampler):\n",
    "        _, preprocess = load_clip()\n",
    "        return FilesReader(\n",
    "            sampler,\n",
    "            preprocess,\n",
    "            folder,\n",
    "            batch_size,\n",
    "            num_prepro_workers,\n",
    "            enable_text=False,\n",
    "            enable_image=True,\n",
    "            enable_metadata=False,\n",
    "        )\n",
    "\n",
    "    def mapper_builder():\n",
    "        return ClipMapper(\n",
    "            enable_image=True,\n",
    "            enable_text=False,\n",
    "            enable_metadata=False,\n",
    "            use_mclip=False,\n",
    "            device=\"cpu\",\n",
    "            clip_model=\"ViT-B/32\",\n",
    "            use_jit=True,\n",
    "            mclip_model=\"\",\n",
    "        )\n",
    "\n",
    "    def writer_builder(i):\n",
    "        return NumpyWriter(\n",
    "            partition_id=i,\n",
    "            output_folder=tmpdir,\n",
    "            enable_text=False,\n",
    "            enable_image=True,\n",
    "            enable_metadata=False,\n",
    "            write_batch_size=10,\n",
    "        )\n",
    "\n",
    "    runner = Runner(\n",
    "        reader_builder=reader_builder,\n",
    "        mapper_builder=mapper_builder,\n",
    "        writer_builder=writer_builder,\n",
    "        output_partition_count=output_partition_count,\n",
    "    )\n",
    "\n",
    "    runner(0)\n",
    "\n",
    "    with open(tmpdir + \"/img_emb/img_emb_0.npy\", \"rb\") as f:\n",
    "        image_embs = np.load(f)\n",
    "        assert image_embs.shape[0] == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next do a standalone distributor test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from clip_retrieval.clip_inference.distributor import PysparkDistributor, SequentialDistributor\n",
    "from clip_retrieval.clip_inference.load_clip import load_clip\n",
    "from clip_retrieval.clip_inference.mapper import ClipMapper\n",
    "from clip_retrieval.clip_inference.reader import FilesReader, WebdatasetReader\n",
    "from clip_retrieval.clip_inference.runner import Runner, Sampler\n",
    "from clip_retrieval.clip_inference.writer import NumpyWriter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import tempfile\n",
    "\n",
    "output_partition_count = 2\n",
    "num_prepro_workers = 8\n",
    "batch_size = 2\n",
    "folder = \"test_images\"\n",
    "distributor_kind = \"pyspark\"\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "\n",
    "    def reader_builder(sampler):\n",
    "        _, preprocess = load_clip()\n",
    "        return FilesReader(\n",
    "            sampler,\n",
    "            preprocess,\n",
    "            folder,\n",
    "            batch_size,\n",
    "            num_prepro_workers,\n",
    "            enable_text=False,\n",
    "            enable_image=True,\n",
    "            enable_metadata=False,\n",
    "        )\n",
    "\n",
    "    def mapper_builder():\n",
    "        return ClipMapper(\n",
    "            enable_image=True,\n",
    "            enable_text=False,\n",
    "            enable_metadata=False,\n",
    "            use_mclip=False,\n",
    "            device=\"cpu\",\n",
    "            clip_model=\"ViT-B/32\",\n",
    "            use_jit=True,\n",
    "            mclip_model=\"\",\n",
    "        )\n",
    "\n",
    "    def writer_builder(i):\n",
    "        return NumpyWriter(\n",
    "            partition_id=i,\n",
    "            output_folder=tmpdir,\n",
    "            enable_text=False,\n",
    "            enable_image=True,\n",
    "            enable_metadata=False,\n",
    "            write_batch_size=10,\n",
    "        )\n",
    "\n",
    "    runner = Runner(\n",
    "        reader_builder=reader_builder,\n",
    "        mapper_builder=mapper_builder,\n",
    "        writer_builder=writer_builder,\n",
    "        output_partition_count=output_partition_count,\n",
    "    )\n",
    "\n",
    "    if distributor_kind == \"sequential\":\n",
    "        distributor = SequentialDistributor(runner, output_partition_count)\n",
    "    elif distributor_kind == \"pyspark\":\n",
    "        from pyspark.sql import SparkSession  # pylint: disable=import-outside-toplevel\n",
    "\n",
    "        spark = (\n",
    "            SparkSession.builder.config(\"spark.driver.memory\", \"16G\")\n",
    "            .master(\"local[\" + str(2) + \"]\")\n",
    "            .appName(\"spark-stats\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "        distributor = PysparkDistributor(runner, output_partition_count)\n",
    "    distributor()\n",
    "\n",
    "    with open(tmpdir + \"/img_emb/img_emb_0.npy\", \"rb\") as f:\n",
    "        image_embs = np.load(f)\n",
    "        assert image_embs.shape[0] == 4\n",
    "    with open(tmpdir + \"/img_emb/img_emb_1.npy\", \"rb\") as f:\n",
    "        image_embs = np.load(f)\n",
    "        assert image_embs.shape[0] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next to an end to end main test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import tempfile\n",
    "\n",
    "from clip_retrieval.clip_inference.main import main\n",
    "\n",
    "num_prepro_workers = 8\n",
    "batch_size = 2\n",
    "input_dataset = \"test_images\"\n",
    "distributor_kind = \"pyspark\"\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    from pyspark.sql import SparkSession  # pylint: disable=import-outside-toplevel\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder.config(\"spark.driver.memory\", \"16G\")\n",
    "        .master(\"local[\" + str(2) + \"]\")\n",
    "        .appName(\"spark-stats\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    main(\n",
    "        input_dataset,\n",
    "        output_folder=tmpdir,\n",
    "        input_format=\"files\",\n",
    "        cache_path=None,\n",
    "        batch_size=256,\n",
    "        num_prepro_workers=8,\n",
    "        enable_text=False,\n",
    "        enable_image=True,\n",
    "        enable_metadata=False,\n",
    "        write_batch_size=4,\n",
    "        wds_image_key=\"jpg\",\n",
    "        wds_caption_key=\"txt\",\n",
    "        clip_model=\"ViT-B/32\",\n",
    "        mclip_model=\"sentence-transformers/clip-ViT-B-32-multilingual-v1\",\n",
    "        use_mclip=False,\n",
    "        use_jit=True,\n",
    "        distribution_strategy=\"pyspark\",\n",
    "        wds_number_file_per_input_file=10000,\n",
    "        output_partition_count=None,\n",
    "    )\n",
    "\n",
    "    with open(tmpdir + \"/img_emb/img_emb_0.npy\", \"rb\") as f:\n",
    "        image_embs = np.load(f)\n",
    "        assert image_embs.shape[0] == 4\n",
    "    with open(tmpdir + \"/img_emb/img_emb_1.npy\", \"rb\") as f:\n",
    "        image_embs = np.load(f)\n",
    "        assert image_embs.shape[0] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next test with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the logger writer and the logger reader here then in a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip_retrieval.clip_inference.logger import LoggerWriter\n",
    "\n",
    "logger = LoggerWriter(partition_id=0, stats_folder=\"/tmp/my_stats\")\n",
    "logger.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    logger(\n",
    "        {\n",
    "            \"read_duration\": 0.5,\n",
    "            \"inference_duration\": 10,\n",
    "            \"write_duration\": 2,\n",
    "            \"total_duration\": 13,\n",
    "            \"sample_count\": 1024,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip_retrieval.clip_inference.logger import LoggerReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = LoggerReader(stats_folder=\"/tmp/my_stats\", enable_wandb=True)\n",
    "reader.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = pd.read_parquet(\"/tmp/my_stats/stats.parquet\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then benchmark it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locally done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then do the doc for pyspark with gpu setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then do the real benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then done"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8890164936ba431effa62f548d2e190a63033d8c51925a70e93a060bef4e9d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
